# GLM3-6B Quantization to INT4

This project focuses on quantizing the GLM3-6B model down to INT4 precision, effectively reducing memory requirements to just 8GB. The quantization process allows the model to maintain high accuracy while being more efficient and accessible for various applications, particularly those with limited hardware resources.

## Features
- **INT4 Quantization:** Reduces the model size significantly while preserving performance.
- **Low Memory Requirement:** Can run on hardware with only 8GB of memory.
- **High Accuracy:** Ensures that the quantized model maintains accuracy comparable to the original.

## Installation
Follow the instructions in the repository to set up the environment and run the quantized model.

## Usage
Detailed usage examples and documentation are provided to help you get started with the INT4 quantized GLM3-6B model.

---

# GLM3-6B 精度降到 INT4

该项目旨在将 GLM3-6B 模型量化到 INT4 精度，有效地将内存需求降低到仅 8GB。量化过程允许模型在提高效率和适应各种应用的同时保持高精度，特别是那些硬件资源有限的应用。

## 特点
- **INT4 量化:** 显著减少模型大小，同时保持性能。
- **低内存需求:** 仅需 8GB 内存即可运行。
- **高精度:** 确保量化模型保持与原始模型相当的精度。

## 安装
按照仓库中的说明设置环境并运行量化后的模型。

## 用法
提供详细的使用示例和文档，帮助您快速上手使用 INT4 量化的 GLM3-6B 模型。
